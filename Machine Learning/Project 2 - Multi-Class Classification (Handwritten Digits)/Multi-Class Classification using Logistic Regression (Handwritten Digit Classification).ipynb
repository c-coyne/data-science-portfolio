{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Using Logistic Regression Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses one-vs-rest multi-class classification to recognize handwritten numbers. This kind of multi-class classification problem can be accomplished rather succintly with pre-built Python libraries and Google API's. This project, however, builds the recognition model from scratch using a vectorized cost and gradient descent functions and a set of logistic regression classifiers. The data used is generated using the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sklearn library to create a labeled dataset of handwritten digits. The dataset includes 1,797 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import sklnear and matplotlib libraries needed\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore convergence warnings of solver using less than the max number of iterations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create our dataset of handwritten digits\n",
    "digits = load_digits()\n",
    "print(\"Dataset dimensions: {}\".format(digits.data.shape))\n",
    "\n",
    "# Convert this dataset into the variables we'll need to build our models\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "num_labels = (np.unique(y)).shape[0]\n",
    "imgs = digits.images\n",
    "\n",
    "# Display one example to visualize what the data looks like\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[1200], fignum=1, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, we can see that each sample is an 8 pixel by 8 pixel grayscale image of a handwritten number. Each pixel represents a floating point number corresponding to the intensity (0 = black up to 16 = white) at that particular location. Let's take a look at the underlying data for the above image of the number 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 12., 16., 16., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6.,  4., 10., 13.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., 13.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  5.,  9., 16., 16., 12.,  0.],\n",
       "       [ 0.,  3., 16., 16., 11.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  7., 13.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 11.,  8.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 16.,  3.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that this is indeed a 7, we can check the label for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the 1199th handwritten digit is 7\n"
     ]
    }
   ],
   "source": [
    "print(\"The label of the {}th handwritten digit is {}\".format(1199, y[1200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have our dataset and our labels. While the data looks one way above, it's actually a matrix with 1,797 rows (corresponding to the number of samples in the dataset) and 64 columns (one for each pixel in an 8-by-8 image). Therefore, our data can be represented by the matrices below, where m = 1,797."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-6 X matrix.PNG\" align=\"center\"/> <img src=\"2020-12-6 Y vector.PNG\" align=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our labels vector, y, numbers 1 through 9 are labeled as such, but 10's are labeled as \"0\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that we're trying to classify these digits into 10 classes, we'll need to train 10 different logistic regression classifiers - one for each digit. Let's start with the hypothesis function for a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-6 hypothesis function.PNG\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit hard to visualize, so let's ensure we know what we're representing here. The sigmoid function can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-7 sigmoid function.PNG\" style=\"width:500px;height:300px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function allows us to determine a probability based on our model parameters and the input data. Briefly, if we build a classifier to detect for the digit \"2\", then we apply our model parameters to the data and determine a resulting value of -2, then our sigmoid function would kick out a value of 0.1192. In our classifier problem, that would mean there's an 11.92% chance that the digit is a \"2\". Conversely, if applying our detect-for-2 model resulted in a value of 4, then our sigmoid function would return 0.982, meaning there's a 98.2% chance that the digit is, in fact, a \"2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hypothesis function established, we can now begin to build the logistic regression model. We'll first build the cost function, then we'll move on to optimizing our cost function. We'll include the standard cost plus a regularization term in the cost function to prevent the model from overfitting the data.\n",
    "\n",
    "Here's the cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-6 cost function w reg.PNG\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot in that one equation, so let's break it down. Remember that this is a classifier problem, so y can only be 0 or 1 (i.e. it *is* a certain digit, and y=1, or it is *not* that digit, and y=0). It cannot be 0.1, 0.5, or anything except 0 or 1. \n",
    "\n",
    "However, our hypothesis function returns a *probability*, meaning that it may get very close to 0 or 1, but it will never actually be 0 or 1. Therefore, it can occupy any value between 0 or 1, such as 0.25. \n",
    "\n",
    "Now, because our y-values are binary, we need a way to calculate the cost for both possible values of y. This is why there are two cost function components above: one for y=1 and one for y=0. Let's visualize those two functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-8 cost component graph.PNG\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two curves above, one for y=1 and one for y=0. Let's look first at y=1. Notice that if our hypothesis function predicts a 1, the cost is zero - it's a perfect prediction, so there is no cost. If our prediction is 0.8, then the cost is larger, but still relatively small because we're close to the target of y=1. If, on the other hand, our hypothesis function predicts 0.1, then we're significantly off the mark, and the cost function ramps up accordingly. \n",
    "\n",
    "It's the same story for y=0, where a prediction of 0 results in no cost, and the cost increases as you move towards 1 and away from the correct value.\n",
    "\n",
    "Finally, we have what I'm calling an \"activator\". This is simply your y-value, and it determines which cost function you use. For each example in our dataset, y either is (y=1) or is not (y=0) the digit in question. Once we know y, we know which cost function to use, and we add it to our total cost per the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we just have our regularization term. This term is added to keep our theta values from growing too large, which would result in overfitting of our dataset. Note that we do *not* regularize the $theta_0$ term, however. We don't want to penalize the model for selecting a large bias term, we just don't want it to emphasize the higher orders in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vectorizing the Cost & Gradient Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of our dataset, our logistic regression classifiers will involve quite a bit of calculation, so we need to use the vectorized form of the cost function since it's so much faster. Here's the vectorized cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-6 vec cost function w reg.PNG\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our cost function, we're also going to include the gradient term so that we can use a built-in optimization function later (we'll use the optimization function instead of gradient descent). Note that the gradient function for our logistic regressions will look very similar to the gradient function in a linear regression, but it is distinct in that the hypothesis function is unique between linear and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2020-12-6 gradient functions.PNG\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def computeCost(theta, x, y, lam):\n",
    "    '''\n",
    "    This is the cost function for a logistic regression model.\n",
    "    This function takes in an nx1 matrix \"theta\", an mxn matrix \"x\", \n",
    "    and an mx1 matrix \"y\", as well as a regularization weighting term\n",
    "    \"lam.\" It then computes and returns the cost and gradient.\n",
    "    '''\n",
    "    \n",
    "    # Set up our m variable (number of samples)\n",
    "    m = len(y)\n",
    "    \n",
    "    # Compute the cost function\n",
    "    # Set up our hypothesis prediction\n",
    "    h = sigmoid(x @ theta)\n",
    "    # Compute the first term, which is the y=1 curve\n",
    "    f_comp = -1*y*np.log(h).flatten()\n",
    "    # Compute the second term, which is the y=0 curve\n",
    "    s_comp = -1*(1-y)*np.log(1-h).flatten()\n",
    "    # Put the two terms together, resulting in the logistic regression cost\n",
    "    j_lr = (1/m) * (np.sum(f_comp) + np.sum(s_comp))    \n",
    "    # Compute the regularization cost term (don't regularize the theta_0 value)\n",
    "    theta_reg = theta[1:]\n",
    "    j_reg = (lam/(2*m)) * np.sum(theta_reg**2)\n",
    "    # Add the regularization cost to the logistic regression cost\n",
    "    j = j_lr + j_reg\n",
    "\n",
    "    # Compute the gradient\n",
    "    g_lr = (1/m) * (x.transpose() @ (h.flatten()-y))\n",
    "    # Compute the gradient regularization parameter\n",
    "    theta_reg = theta[1:]\n",
    "    g_reg = (lam/m) * np.sum(theta_reg)\n",
    "    # Add regularization to the gradient\n",
    "    grad = g_lr + g_reg    \n",
    "    \n",
    "    # Return the cost and gradient\n",
    "    return j, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one more function to write, and that's our one-vs-rest function. This function will use a built-in minimization function from the scipy.optimize library to simultaneously adjust all theta values towards their optimal settings. We could use gradient descent, but the minimize function is more efficient for larger-dimension theta vectors, like the one we have in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def oneVsRest(x, y, num_labels, lam):\n",
    "\n",
    "    # Set up our variables n (number of parameters) and m (number of samples)\n",
    "    m, n = x.shape\n",
    "\n",
    "    # Initialize our final theta values as an (num_labels)x(n+1) array of 0's\n",
    "    final_theta = np.zeros((num_labels, n+1))\n",
    "\n",
    "    # Add a column of ones to the x data matrix to represent x0 values\n",
    "    x = np.column_stack((np.ones((m,1)), x))\n",
    "    \n",
    "    # Use the data to train logistic regression models\n",
    "    # We have 10 classes in this project\n",
    "    for c in range(num_labels):\n",
    "        \n",
    "        # Start with a theta guess of all zeros\n",
    "        initial_theta = np.zeros((n+1, 1))\n",
    "        print(\"Training {:d} out of {:d} categories... \".format(c+1, num_labels))\n",
    "        \n",
    "        # Minimize the cost function by adjusting the theta vector\n",
    "        myargs = (x, (y%10==c).astype(int), 0.1)\n",
    "        theta = minimize(computeCost, x0=initial_theta, args=myargs, \n",
    "                         options={'disp':True, 'maxiter':13}, method=\"Newton-CG\", jac=True)\n",
    "\n",
    "        # Assign a new row to all_theta for class c\n",
    "        final_theta[c,:] = theta[\"x\"]\n",
    "    \n",
    "    # Return the final trained theta values\n",
    "    return final_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We have the backbone built, so let's train our models. First, we'll split our full dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Break up the dataset into a training set (80% of the data) and a test set (20% of the data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)\n",
    "\n",
    "# Do the same for the images dataset. This will allow us to visualize handwritten numbers later\n",
    "imgs_train, imgs_test = train_test_split(imgs, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model using the functions we've built. To recap, we'll pass our training data into our oneVsRest function, which uses the scipy.optimize built-in function \"minimize\" to minimize the cost by adjusting the theta parameter. The oneVsRest function therefore must call our cost function, which returns the new theta values and the gradient for each iteration. Continuing to follow the train backwards, the cost function is built on our hypothesis function, which uses the sigmoid function to determine a probability value. All that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000243\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 147\n",
      "         Hessian evaluations: 0\n",
      "Training 2 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.029187\n",
      "         Iterations: 13\n",
      "         Function evaluations: 17\n",
      "         Gradient evaluations: 109\n",
      "         Hessian evaluations: 0\n",
      "Training 3 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000359\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 112\n",
      "         Hessian evaluations: 0\n",
      "Training 4 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.019475\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 107\n",
      "         Hessian evaluations: 0\n",
      "Training 5 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.001907\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 96\n",
      "         Hessian evaluations: 0\n",
      "Training 6 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.002144\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 131\n",
      "         Hessian evaluations: 0\n",
      "Training 7 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.001739\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 123\n",
      "         Hessian evaluations: 0\n",
      "Training 8 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: nan\n",
      "         Iterations: 13\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 158\n",
      "         Hessian evaluations: 0\n",
      "Training 9 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.084613\n",
      "         Iterations: 13\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 120\n",
      "         Hessian evaluations: 0\n",
      "Training 10 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.028506\n",
      "         Iterations: 13\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 100\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "# Establish two input parameters\n",
    "num_labels = (np.unique(y)).shape[0]\n",
    "lam = 0.1\n",
    "\n",
    "# Train the model\n",
    "final_theta = oneVsRest(x_train, y_train, num_labels, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't stop there, or we won't know how well training went. We need to *test* the model! \n",
    "\n",
    "Remember we partitioned out 20% of our data to be used specifically for testing. Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_num(theta, x):\n",
    "    \n",
    "    # Set up our m (number of samples) and c (# of classes) variables\n",
    "    m = x.shape[0]\n",
    "    c = theta.shape[0]\n",
    "\n",
    "    # Create an mx1 vector of zeroes to hold probability values\n",
    "    p = np.zeros((m,1))\n",
    "\n",
    "    # Add our x0 column to the x array\n",
    "    x = np.column_stack((np.ones((m,1)), x))\n",
    "\n",
    "    # Use the argmax function to identify the most likely class per training example\n",
    "    h = sigmoid(x @ theta.T)\n",
    "    p = np.argmax(h, axis=1)\n",
    "    #p = np.argmax(sigmoid(np.dot(x, theta.T)), axis=1)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = predict_num(final_theta, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell used the model that we built to identify the digits for all of our test dataset. Let's look at the accuracy to see how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAccuracy(x_pred, y, num_labels):\n",
    "\n",
    "    # Create a list to keep track of the number correct and the total number of examples\n",
    "    scores, counts = [0]*num_labels, [0]*num_labels\n",
    "    score_overall, count_overall = 0, 0\n",
    "    \n",
    "    # Divide the dataset up into the separate digits and add scores / counts to lists\n",
    "    for label in range(num_labels):\n",
    "        \n",
    "        # Find the indices where the label matches the digit of interest\n",
    "        y_lab = np.argwhere(y==label).tolist()\n",
    "        flat_list = [item for sublist in y_lab for item in sublist]\n",
    "        \n",
    "        # Tally up the total number of instances as well as the number of matches\n",
    "        for item in flat_list:\n",
    "            if x_pred[item] == y[item]%10:\n",
    "                scores[label] += 1\n",
    "                score_overall += 1\n",
    "            counts[label] += 1\n",
    "            count_overall += 1\n",
    "            \n",
    "    # Calculate percent accuracies\n",
    "    acc_by_num = [0]*num_labels\n",
    "    for item in range(len(scores)):\n",
    "        acc_by_num[item] = scores[item]/counts[item]\n",
    "    \n",
    "    # Determine the overall accuracy\n",
    "    acc_overall = score_overall / count_overall\n",
    "    \n",
    "    return acc_by_num, acc_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_num, acc_overall = calcAccuracy(x_pred, y_test, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test set accuracy: 95.00%\n",
      "Test set accuracy for 1: 87.23%\n",
      "Test set accuracy for 2: 100.00%\n",
      "Test set accuracy for 3: 94.12%\n",
      "Test set accuracy for 4: 97.37%\n",
      "Test set accuracy for 5: 100.00%\n",
      "Test set accuracy for 6: 100.00%\n",
      "Test set accuracy for 7: 94.29%\n",
      "Test set accuracy for 8: 94.44%\n",
      "Test set accuracy for 9: 87.50%\n",
      "Test set accuracy for 10: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy first\n",
    "print(\"Overall test set accuracy: {:.2%}\".format(acc_overall))\n",
    "\n",
    "# Print the accuracy by digit\n",
    "for i in range(1,num_labels):\n",
    "    print(\"Test set accuracy for {}: {:.2%}\".format(i, acc_by_num[i]))\n",
    "print(\"Test set accuracy for 10: {:.2%}\".format(acc_by_num[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but not great. Overall, our accuracy was 95.00%, where individual accuracies varied by digit. The number 1 was our lowest accuracy at 87.23%. While we won't go into an extensive diagnosis and evaluation, we can conduct a simple test to determine if our low accuracy values are due to (1) an overtrained model or (2) not enough data. If we train the very same model on a different split of the data (ex. 90/10 split of training to test vs. the 80/20 split we used above), let's see what accuracy numbers we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000220\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 147\n",
      "         Hessian evaluations: 0\n",
      "Training 2 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.038672\n",
      "         Iterations: 13\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 104\n",
      "         Hessian evaluations: 0\n",
      "Training 3 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000406\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 124\n",
      "         Hessian evaluations: 0\n",
      "Training 4 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.020828\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 113\n",
      "         Hessian evaluations: 0\n",
      "Training 5 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.001262\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 94\n",
      "         Hessian evaluations: 0\n",
      "Training 6 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.002582\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 125\n",
      "         Hessian evaluations: 0\n",
      "Training 7 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.003963\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 117\n",
      "         Hessian evaluations: 0\n",
      "Training 8 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000776\n",
      "         Iterations: 13\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 161\n",
      "         Hessian evaluations: 0\n",
      "Training 9 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.087986\n",
      "         Iterations: 13\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 120\n",
      "         Hessian evaluations: 0\n",
      "Training 10 out of 10 categories... \n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.027489\n",
      "         Iterations: 13\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 150\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "# Break up the dataset into a training set (90% of the data) and a test set (10% of the data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=40)\n",
    "\n",
    "# Do the same for the images dataset. This will allow us to visualize handwritten numbers\n",
    "imgs_train, imgs_test = train_test_split(imgs, test_size=0.1, random_state=40)\n",
    "\n",
    "# Establish two input parameters\n",
    "num_labels = (np.unique(y)).shape[0]\n",
    "lam = 0.1\n",
    "\n",
    "# Train the model\n",
    "final_theta = oneVsRest(x_train, y_train, num_labels, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test set accuracy: 96.67%\n",
      "Test set accuracy for 1: 88.00%\n",
      "Test set accuracy for 2: 100.00%\n",
      "Test set accuracy for 3: 92.86%\n",
      "Test set accuracy for 4: 95.65%\n",
      "Test set accuracy for 5: 100.00%\n",
      "Test set accuracy for 6: 100.00%\n",
      "Test set accuracy for 7: 100.00%\n",
      "Test set accuracy for 8: 100.00%\n",
      "Test set accuracy for 9: 94.12%\n",
      "Test set accuracy for 10: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict handwritten digit classifications\n",
    "x_pred = predict_num(final_theta, x_test)\n",
    "\n",
    "# Calculate the accuracy of our updated model\n",
    "acc_by_num, acc_overall = calcAccuracy(x_pred, y_test, num_labels)\n",
    "\n",
    "# Print the overall accuracy first\n",
    "print(\"Overall test set accuracy: {:.2%}\".format(acc_overall))\n",
    "\n",
    "# Print the accuracy by digit\n",
    "for i in range(1,num_labels):\n",
    "    print(\"Test set accuracy for {}: {:.2%}\".format(i, acc_by_num[i]))\n",
    "print(\"Test set accuracy for 10: {:.2%}\".format(acc_by_num[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. The above accuracies tell us that we're simply lacking data. All of our accuracies improved by shifting from an 80/20 split to a 90/10 split, which would indicate that a much larger dataset would allow for a much better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up the cell below to allow us to visualize data from the test set, see what our model predicts, and see what the actual label is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while True:\n",
    "    response = input(\"Pick a sample to observe, from 0 to {}: \".format(x_test.shape[0]))\n",
    "    clear_output()\n",
    "    try:\n",
    "        sample = int(response)\n",
    "        plt.figure(figsize=(2,2))\n",
    "        plt.gray()\n",
    "        plt.matshow(imgs_test[sample], fignum=1, aspect='auto')\n",
    "        plt.show()\n",
    "        print(\"The prediction is: {}\".format(x_pred[sample]))\n",
    "        print(\"The label is: {}\".format(y_test[sample]))\n",
    "    except:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
